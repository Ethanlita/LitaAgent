# **HRL-X架构深度剖析：SCML 2025环境下的分层强化学习、决策Transformer与离线RL协同机制研究**

## **1\. 执行摘要：从启发式规则到认知型架构的范式转移**

供应链管理联赛（Supply Chain Management League, SCML）作为国际自动协商代理竞赛（ANAC）中最具挑战性的赛道之一，长期以来被视为检验多智能体系统（Multi-Agent Systems, MAS）在复杂经济环境中适应能力的试金石。随着2025年标准联赛（Standard League）规则的重大迭代，特别是由于非易腐库存（Non-Perishable Inventory）机制的引入、期货合约（Future Contracts）交易的开放以及更为严苛的短缺惩罚（Shortfall Penalties）体系的确立 1，竞赛的本质发生了根本性的转变。它不再仅仅是一个考察单次博弈谈判技巧的竞技场，而是演变成了一个要求代理具备长周期库存规划、跨期风险对冲以及多线程并发资源协调能力的综合性企业资源规划（ERP）模拟环境。

回顾2024年及之前的赛果，以 PenguinAgent 和 AS0 为代表的基于规则的启发式代理（Heuristic Agents）凭借其稳健的成本控制逻辑和确定性的风险规避策略，占据了排行榜的前列 1。然而，这种“硬编码”的智慧在面对2025年日益复杂的动态市场时显露出了明显的局限性：它们缺乏对非平稳市场环境的自适应能力，难以利用长尾分布中的投机机会，且无法有效处理由期货交易带来的状态空间指数级膨胀问题。为了突破这一策略天花板，学术界与工业界开始将目光投向更具认知深度的强化学习（Reinforcement Learning, RL）架构。

本研究报告深入探讨了针对SCML 2025环境定制的“混合残差学习架构”（Hybrid Residual Learner, HRL-X）。该架构并非单一算法的简单堆砌，而是一个有机的控制论系统，它通过 **分层强化学习（HRL）** 解决时间尺度的抽象与长期规划问题，利用 **决策Transformer（Decision Transformer, DT）** 处理博弈过程中的序列依赖与对手建模，并借助 **离线强化学习（Offline RL）** 中的 "Reward-on-the-Line" (ROL) 机制解决高风险环境下的探索安全与样本效率问题 1。本报告将详尽阐述这三大核心技术模块的具体问题解决机制，并深入剖析在高维、稀疏奖励环境下如何设计有效的奖励信号处理体系，旨在为开发下一代超人级供应链代理提供详实的理论依据与实践蓝图。

## **2\. 环境解构：SCML 2025 的部分可观察马尔可夫决策过程 (POMDP) 特性**

在深入讨论算法架构之前，必须首先对 SCML 2025 的博弈环境进行严格的数学与经济学解构。该环境在本质上是一个具有高维状态空间、连续动作空间以及延迟奖励特征的部分可观察马尔可夫决策过程（POMDP）。

### **2.1 状态空间的非完全可观察性与信息不对称**

在 POMDP 框架 $\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\mathcal{R}, \\Omega, \\mathcal{O}, \\gamma \\rangle$ 中，代理面临的首要挑战是全局状态 $\\mathcal{S}$ 的不可知性。SCML 的全局状态包含了上帝视角下的所有市场信息：每一个竞争对手的私有库存水平、资金状况、生产线闲置率、隐藏的生产成本函数，以及未来 $T$ 天的真实市场供需曲线 1。对于单一代理而言，这些关键信息是完全屏蔽的。

代理所能依赖的仅是极其有限的观测空间 $\\Omega$。这包括自身的私有状态（如当前库存 $I\_t$、资金余额 $B\_t$）以及部分带有噪声或滞后性的公共市场状态（如公告板上的交易价格指数、季度财务报告）1。这种严重的信息不对称导致了非平稳性（Non-Stationarity）：从代理的局部视角看，环境的转移概率 $\\mathcal{T}$ 似乎随时间变化，因为对手的策略在变，而这些变化无法被直接观测。因此，任何有效的算法必须具备 **信念状态重构（Belief State Reconstruction）** 的能力，即通过观测到的历史交互序列（如对手的报价历史 $H\_k$），推断出隐藏状态（如对手的急迫程度或底线价格）。

### **2.2 2025年规则变革引发的“三个耦合难题”**

2025年的规则变更并非简单的参数调整，而是引入了深刻的结构性变化，导致了三个核心优化问题的强耦合，使得传统方法难以应对。

#### **2.2.1 库存控制与跨期套利的耦合**

非易腐库存规则的引入 1 彻底改变了库存管理的逻辑。在旧规则下，库存每日清零，代理只需关注当天的供需平衡（Just-In-Time）。而在新规则下，库存可以累积，这引入了 **跨期套利（Inter-temporal Arbitrage）** 的可能性。代理可以在原材料价格低廉时大量囤积，待市场价格回升后由成品售出。这要求代理解一个动态报童问题（Dynamic Newsvendor Problem），在持有成本（Storage Cost）、资金占用成本（Opportunity Cost）和未来的缺货惩罚（Shortfall Penalty）之间寻找复杂的平衡点。这种决策具有极长的因果链条：今日的买入动作可能旨在满足十天后的市场需求，导致奖励信号在时间维度上极度稀疏且延迟。

#### **2.2.2 并发协商与资源争夺的耦合**

SCML 2025 强调并发协商（Concurrent Negotiation）1。代理必须同时在多个线程中与上游供应商和下游采购商进行谈判。这些看似独立的谈判线程实际上通过共享的资源池（有限的库存空间、有限的现金流、有限的产能）紧密耦合。

* **资金耦合**：如果在上游过度激进地锁定原材料，可能导致现金流枯竭，无法支付下游交易的违约金。  
* 库存耦合：如果下游销售谈判失败，上游源源不断运来的原料将导致爆仓，产生高额仓储费甚至被迫销毁。  
  这种耦合性要求策略必须是 全局感知（Globally Aware） 的，而非局部贪婪。单一线程的最优解（例如以极低价格买入大量原料）如果缺乏下游需求的配合，可能对全局而言是灾难性的。

#### **2.2.3 风险厌恶与探索的耦合**

取消现货市场并引入严厉的短缺惩罚 1 使得环境具有极端的风险不对称性。对于 RL 代理而言，这意味着 **探索（Exploration）** 的代价极其高昂。在训练初期，基于 $\\epsilon$-greedy 或高熵策略的随机探索极易触发连续违约，导致代理破产并提前终止回合。这种“死亡陷阱”使得代理难以收集到成功的正样本，陷入次优的保守策略（即完全不交易）。HRL-X 必须引入某种机制来解耦“安全生存”与“策略优化”。

## **3\. HRL-X 架构核心：混合残差学习的具体机制**

针对上述挑战，HRL-X（Hybrid Residual Learner）架构被提出。其核心设计哲学是 **“带盾牌的残差学习”（Shielded Residual Learning）**。该架构不试图让神经网络从零开始学习复杂的供应链规则，而是将一个基于专家规则的稳健策略作为基准（Baseline），让神经网络专注于学习相对于该基准的 **优化残差（Optimization Residual）**。

### **3.1 架构的拓扑结构**

HRL-X 系统在逻辑上划分为三个层级，分别对应不同的认知深度与控制频率：

| 层级名称 | 核心技术 | 运行频率 | 功能定位 |
| :---- | :---- | :---- | :---- |
| **L1: 安全基底层 (Safety Shield)** | 启发式规则 (PenguinAgent Logic) | 实时 (Real-time) | 生成动作掩码，确保生存，提供基准动作。 |
| **L2: 残差优化层 (Residual Actor)** | 决策Transformer (Decision Transformer) | 每轮 (Per Round) | 基于序列上下文，输出相对于基准的动作偏置。 |
| **L3: 战略规划层 (Strategic Manager)** | 分层强化学习 (Hierarchical RL) | 每天 (Per Day) | 设定宏观库存与资金目标，指导L2的微观决策。 |

### **3.2 安全基底层的问题解决机制**

安全基底层的存在直接解决了 冷启动（Cold Start） 和 探索风险 问题。  
该层复用了 2024 年冠军代理 PenguinAgent 的核心逻辑 1。PenguinAgent 的策略特点是“基于精确计算的成本极小化”和“需求逆推”。它通过计算 $Q\_{buy} \= Q\_{capacity} \- I\_{current}$ 来严格控制采购量，绝不进行投机性囤货。  
在 HRL-X 中，该层并不直接控制代理，而是发挥两个关键作用：

1. **动作掩码生成（Action Masking）**：它基于当前的库存约束、资金约束和已承诺的合约，计算出一个 **可行动作空间集合** $\\mathcal{A}\_{safe} \\subseteq \\mathcal{A}$。例如，如果当前空闲库存仅剩 10 单位，任何数量 $q \> 10$ 的买入报价都会被掩码（Mask）屏蔽，分配为 $-\\infty$ 的概率。这从根本上杜绝了 RL 代理因随机探索导致的爆仓或违约风险，解决了“安全性”问题。  
2. **基准动作提供（Baseline Proposal）**：它输出一个基准动作 $a\_{base}$（例如，“建议报价 100 元”）。这为 RL 网络提供了一个高质量的初始点，使得学习过程变成了“微调”而非“从头摸索”，极大提升了样本效率。

### **3.3 残差优化层的问题解决机制**

残差优化层是智能的核心，采用 **决策Transformer（Decision Transformer, DT）**。其具体的解决机制如下：

#### **3.3.1 解决静态策略的僵化问题**

PenguinAgent 等规则代理虽然稳健，但极为僵化，无法利用对手的弱点。残差层通过输出一个偏置项 $\\Delta a$ 来修正基准动作：

$$a\_{final} \= \\text{Clip}(a\_{base} \+ \\Delta a, \\mathcal{A}\_{safe})$$

如果 DT 识别出当前对手是一个急于出货的“软弱”对手，它会输出一个负向的价格偏置（例如 $\\Delta p \= \-5$），从而在保证成交的前提下压低成本。这种 残差学习机制 使得代理既保留了规则的底线安全性，又具备了 RL 的机会主义灵活性。

#### **3.3.2 解决序列上下文的理解问题**

协商本质上是序列博弈。对手的每一个动作都蕴含信息。传统的 RL（如 DQN）通常只看当前状态快照。HRL-X 利用 Transformer 的 **自注意力机制（Self-Attention）** 处理长达数十轮的报价历史序列 $H\_k \= \\{o\_{t-N}, \\dots, o\_t\\}$。

* **机制**：Transformer 将报价序列映射为高维嵌入（Embedding），通过多头注意力（Multi-Head Attention）捕捉对手报价的时间模式。例如，如果对手的让步曲线（Concession Curve）呈现指数级下降，模型能识别出这是“时间压力大”的信号，从而指导代理在最后时刻保持强硬，榨取更多剩余价值。  
* **解决**：这解决了 POMDP 中对隐藏状态（对手心理）的推断问题。

## **4\. 分层强化学习 (HRL)：时空抽象与长期信用分配**

在 SCML 2025 中，时间尺度的二元性（天 vs. 轮）导致了严重的信用分配难题。HRL-X 引入了 **“管理者-执行者”（Manager-Worker）** 架构 2 来具体解决这一问题。

### **4.1 管理者 (Manager)：解决库存规划与跨期套利**

管理者运行在 **“天”（Day）** 的时间尺度上。

* **输入状态**：宏观经济指标，包括总库存 $I\_{total}$、总资金 $B\_{total}$、市场平均价格指数 $P\_{index}$、未来 $H$ 天的订单承诺量。  
* 动作空间（目标设定）：管理者不直接报价，而是设定当天的 战略目标向量 (Goal Vector) $g\_t$。

  $$g\_t \= \[Q\_{target\\\_buy}, P\_{limit\\\_buy}, Q\_{target\\\_sell}, P\_{limit\\\_sell}\]$$

  例如，管理者可能发出指令：“今天必须买入 50 单位原料，且均价不得高于 110 元；同时卖出所有成品”。  
* **具体解决机制**：  
  * **降维打击**：通过将决策频率从“每轮”降低到“每天”，管理者将整个赛季的决策步数从几十万步压缩到了几百步。这使得 RL 算法（如 PPO）能够有效地学习跨越数十天的长期策略，如“在第 10 天低价囤货，在第 20 天高价抛售”。  
  * **解决信用分配**：管理者直接针对当天的净利润和库存健康度获得奖励。这种低频、高质的反馈信号使得模型能够理解“囤货”这一行为的长期价值，从而有效利用 2025 年的非易腐库存规则进行套利。

### **4.2 执行者 (Worker)：解决微观谈判博弈**

执行者运行在 **“轮”（Round）** 的时间尺度上。

* **输入状态**：除了微观的谈判上下文（对手历史报价），还必须接收管理者的指令 $g\_t$ 作为条件输入。  
* **具体解决机制**：  
  * **目标导向生成**：执行者利用 Decision Transformer，根据 $g\_t$ 生成具体的报价动作。如果管理者设定了“买入 50 单位”的目标，执行者就会在谈判中表现得更为积极（Aggressive）；如果目标是“清理库存”，执行者就会大幅降价。  
  * **解耦策略**：执行者不需要关心长期的库存风险（这是管理者的责任），它只需专注于“如何以最低成本完成管理者交代的买入任务”。这种 **认知负荷的分离** 极大地简化了微观策略的学习难度。

## **5\. 离线强化学习 (Offline RL)：数据驱动的策略预训练**

SCML 提供了包含历年比赛数据的丰富日志库 scml-agents 1。HRL-X 利用离线 RL 技术，特别是 **"Reward-on-the-Line" (ROL)** 算法，解决了在线训练昂贵且不稳定的问题。

### **5.1 具体解决机制：对抗分布偏移 (Distribution Shift)**

离线 RL 的核心挑战是 **分布偏移**。当策略试图执行一个数据集中未见的动作（Out-of-Distribution, OOD）时，Q值网络往往会给出错误的过高估计，导致策略崩溃。

#### **5.1.1 集合一致性约束 (Ensemble Agreement)**

HRL-X 采用了 ROL 算法中的集合机制 1。系统训练 $N$ 个独立的 Q 值网络（Ensemble）。

* **机制**：对于任何给定的状态-动作对 $(s, a)$，计算这 $N$ 个网络输出的 Q 值的方差 $\\text{Var}(Q\_i(s, a))$。  
* **逻辑**：对于数据集内的常见动作，所有网络见过的样本多，输出的 Q 值会趋于一致（低方差）。对于 OOD 动作，由于缺乏数据约束，不同网络的推测会天差地别（高方差）。  
* 应用：我们将 Q 值方差作为 不确定性惩罚项 引入目标函数：

  $$Q\_{target}(s, a) \= \\min\_{i} Q\_i(s, a) \- \\lambda \\cdot \\text{Var}(Q\_i(s, a))$$

  这迫使代理在离线训练阶段自动避开那些不确定的区域，只在有数据支持的安全范围内优化策略。

#### **5.1.2 加权行为克隆 (Weighted Behavior Cloning)**

ROL 还结合了优势加权的行为克隆。系统并不盲目模仿数据集中的所有行为（因为其中包含了失败者的糟糕操作），而是根据 **回报优势（Advantage）** 对样本进行加权。

* **机制**：只模仿那些 $A(s, a) \> 0$ 的样本，即比平均水平表现更好的动作。  
* **解决**：这使得 HRL-X 能够从 PenguinAgent 的历史数据中提取精华，同时自动过滤掉其在极端情况下的失误，实现“青出于蓝而胜于蓝”。

## **6\. 奖励信号处理与整形研究**

在 SCML 2025 中，原始的奖励信号（最终利润）具有极度的稀疏性和延迟性。为了引导 HRL-X 高效收敛，必须设计一套复杂的 **复合奖励整形（Reward Shaping）** 机制。

### **6.1 复合奖励函数架构**

HRL-X 采用如下形式的奖励函数 $R\_t$：

$$R\_t \= R\_{profit} \+ \\lambda\_1 R\_{liquidity} \- \\lambda\_2 R\_{risk} \+ \\lambda\_3 R\_{intrinsic}$$

#### **6.1.1 潜在价值势能 ($R\_{profit}$ 与 Potential Function)**

直接使用现金流作为奖励会导致短视（只愿卖不愿买）。HRL-X 引入 势能函数（Potential Function） $\\Phi(s)$ 来评估库存的潜在价值。

$$\\Phi(s) \= I\_{inventory} \\times P\_{market\\\_avg}$$

$$R\_{profit} \= \\text{Cash}\_{t+1} \- \\text{Cash}\_t \+ \\gamma \\Phi(s\_{t+1}) \- \\Phi(s\_t)$$

* **机制**：当代理买入原材料时，虽然现金减少了，但库存势能 $\\Phi(s)$ 增加了。根据奖励塑形定理（Reward Shaping Theorem），总回报保持不变，但代理能获得即时的正向反馈，从而学会“投资”行为。

#### **6.1.2 流动性激励 ($R\_{liquidity}$)**

针对新手代理容易陷入“零交易陷阱”（因害怕亏损而拒绝所有交易）的问题，引入流动性奖励 1。

* **机制**：只要达成任何一笔交易，无论利润微薄与否，都给予一个微小的正向奖励 $\\epsilon$。  
* **解决**：这鼓励代理保持市场活跃度，确保持续收集对手数据，防止策略坍缩到静止状态。

#### **6.1.3 前瞻性风险惩罚 ($R\_{risk}$)**

针对短缺惩罚，不能等到违约发生时才惩罚（那时为时已晚）。

* 机制：基于当前合约和库存的预测模型，如果预测未来 $t+H$ 天的库存 $I\_{future} \< 0$，立即施加指数级增长的负奖励。

  $$R\_{risk} \= \- \\beta \\cdot \\exp(\\max(0, \-I\_{future}))$$  
* **解决**：这迫使代理在危机发生的早期阶段（Early Warning）就采取激进的补救措施（如高价买入原料），将风险扼杀在摇篮中。

#### **6.1.4 内在一致性奖励 ($R\_{intrinsic}$)**

这是连接 Manager 和 Worker 的纽带。

* 机制：Worker 的奖励不仅取决于成交，还取决于 与 Manager 目标的对齐度。

  $$R\_{intrinsic} \= \- \\| q\_{executed} \- q\_{goal} \\|^2$$  
* **解决**：这防止了 Worker 为了贪图眼前的小利而破坏了 Manager 的宏观库存布局。

## **7\. 并发协商与全局协调器的具体解决机制**

SCML 2025 的并发协商要求代理在多个线程间分配注意力。HRL-X 在第三层引入了 **全局协调器（Global Coordinator）**。

### **7.1 基于注意力的动态权重分配**

协调器本质上是一个 **多头自注意力网络（Multi-Head Self-Attention Network）**。

* **输入**：所有活跃谈判线程 $k$ 的隐状态向量 $h\_k$（由各线程的 Decision Transformer 提取）。  
* 机制：协调器计算每个线程的 重要性权重（Attention Weight） $\\alpha\_k$。

  $$\\alpha \= \\text{Softmax}(\\frac{Q K^T}{\\sqrt{d\_k}})$$

  其中 $Q$ 是全局状态的查询向量，$K$ 是各线程的键向量。  
* **逻辑**：如果系统检测到库存告急，且线程 A 是唯一能提供原料的渠道，注意力机制会自动赋予线程 A 极高的权重 $\\alpha\_A$。

### **7.2 策略修正与资源倾斜**

计算出的权重 $\\alpha\_k$ 直接影响各线程 Worker 的决策逻辑：

* **高权重线程（关键路径）**：对于 $\\alpha\_k$ 高的线程，系统会 **放宽利润约束**。残差层会接收到一个信号，使其倾向于输出大幅度的让步动作（如提高买价），以确保 100% 成交。  
* 低权重线程（备选路径）：对于 $\\alpha\_k$ 低的线程，系统会采取 高风险高回报（High-Risk High-Return） 策略，尝试报出极具侵略性的价格。如果成交则赚取暴利，不成交也无损大局。  
  这种机制模仿了人类谈判专家的 战略聚焦 能力，解决了多线程资源争夺中的帕累托优化问题。

## **8\. 结论与实施建议**

SCML 2025 的规则变革将供应链博弈推向了新的认知高度。本报告提出的 HRL-X 架构，通过 **HRL 的时空分层** 解决了长期库存规划与短期博弈的矛盾，通过 **Decision Transformer 的序列建模** 实现了对非平稳对手的深度感知，通过 **离线 RL 与 ROL 机制** 解决了高风险环境下的冷启动与安全性问题，并配合 **精细的奖励整形** 与 **全局注意力机制**，构建了一套理论完备且极具实战潜力的解决方案。

对于参赛团队，实施 HRL-X 的路线图清晰可见：首先利用 PenguinAgent 构建安全基底与数据生成器；其次利用离线日志预训练 Decision Transformer；最后引入分层 Manager 进行宏观策略微调。这一架构不仅有望在 SCML 2025 中夺魁，更为解决现实世界中复杂的供应链自动化谈判问题提供了通用的算法范式。

---

*(报告结束)*

#### **引用的著作**

1. SCML 2025 代理开发策略研究.docx  
2. SCML 2025 代理开发策略研究