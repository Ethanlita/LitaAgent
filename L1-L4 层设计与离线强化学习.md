# **SCML 2025 供应链多智能体架构 HRL-X 深度研究报告：分层设计、离线强化学习与动态博弈策略**

**摘要**

随着 2025 年国际供应链管理联赛（SCML）引入非易腐库存、期货合约及高额短缺惩罚等颠覆性规则，传统的单一启发式代理或扁平化强化学习（RL）模型已难以应对该环境下的高维状态空间与极度稀疏的奖励信号。本报告针对 HRL-X（Hybrid Residual Learner \- Extended）架构的核心设计难题进行详尽的理论推导与工程验证，重点解决 L1 安全基底层的构建逻辑、L2/L3 层级的动态博弈对象选择机制，以及基于“事后诸葛亮”机制（Hindsight Logic）的双层离线强化学习训练范式。报告全文约 15,000 字，旨在为构建具备超人级表现的供应链智能体提供完备的技术蓝图。

## ---

**1\. 引言：SCML 2025 的认知挑战与架构演进**

供应链管理联赛（SCML）的标准赛道（Standard Track）在 2025 年迎来了其历史上最深刻的一次规则重构。这些变化不仅是参数层面的调整，更是环境底层数学性质的质变。理解这些变化是回答关于 L1、L2 及离线 RL 设计问题的先决条件。

### **1.1 从单次博弈到长期规划的范式转移**

在早期的 SCML 赛事中，代理面临的往往是“报童问题”的变种：每天库存清零，决策仅需关注当日的边际收益。然而，SCML 2025 引入了**非易腐库存（Non-Perishable Inventory）和期货合约（Futures Contracts）**1。

* **状态空间的非马尔可夫化**：当前的决策（如买入原材料）不再仅影响当天的回报，而是通过库存这一“记忆变量”影响未来数十天的状态。这使得环境具有了深度的长程相关性（Long-term Dependency）。  
* **奖励信号的极度延迟**：代理可能在第 1 天支付现金买入原料（负奖励），在第 5 天生产，第 10 天售出（正奖励）。这种长达 10 天的信用分配链条（Credit Assignment Gap）使得传统的 Q-Learning 算法难以捕捉动作与后果的因果联系。

### **1.2 耦合难题：架构设计的原点**

本报告提出的 HRL-X 架构，其核心目的在于解耦 SCML 2025 中的三个核心矛盾（Triple Coupling）2：

1. **安全与探索的耦合**：在严厉的短缺惩罚下，随机探索（Exploration）意味着破产。这直接导致了对 L1 层设计的特殊要求——必须是确定性的安全护盾。  
2. **战略与战术的耦合**：宏观的库存规划（每天一次）与微观的谈判博弈（每轮一次）在时间尺度上相差数个数量级。这决定了 L2 与 L3 的分层结构。  
3. **并发与资源的耦合**：同时进行的数十场谈判争夺有限的资金和产能。这引出了关于“谈判对象选择”的深刻讨论——本质上是注意力资源的分配问题。

## ---

**2\. L1 安全基底层（Safety Shield）：设计哲学与实现细节**

针对用户提出的第一个核心问题：“**L1层的设计究竟应该是什么样子的？这一层应该完全基于启发式代理吗？**”，本章将从控制理论与强化学习稳定性的角度给出确定的回答。

### **2.1 为什么 L1 必须是确定性启发式规则？**

在 HRL-X 架构中，L1 层被称为“安全护盾”（Safety Shield）。其设计原则是**不可学习性（Non-Learnability）和确定性（Determinism）**。

#### **2.1.1 神经网络的概率陷阱**

如果在 L1 层使用 Transformer 或任何基于梯度下降的神经网络，我们将面临两个致命风险：

1. **冷启动灾难（Cold Start Catastrophe）**：在训练初期，神经网络的权重接近随机初始化。在一个没有约束的动作空间中，代理极大概率会报出违反物理约束的价格（如 $P \> \\text{Balance}$）或数量（$Q \> \\text{Capacity}$）。在 SCML 2025 中，这种错误不是带来低分，而是直接导致破产（Bankruptcy）或被取消资格。  
2. **分布外泛化失效（OOD Failure）**：即使网络训练良好，当市场环境进入训练集中未曾见过的极端状态（如恶性通胀）时，神经网络可能会产生“幻觉”动作。

#### **2.1.2 启发式规则的“宪法”地位**

因此，L1 层必须基于严格的领域逻辑——即 2024 年冠军 PenguinAgent 的核心算法 3。它不是用来“优化”利润的，而是用来“定义”可行域的。它充当了代理的“宪法”，任何上层（L2/L3）的指令如果违宪（即违反 L1 的约束），都将被强制否决。

### **2.2 L1 的双重输出：动作掩码与基准动作**

用户询问：“如果基于Transformer，那么这一层是不是只要产出动作掩码就可以了，而不需要产出基准动作？”  
回答是：L1 应该基于启发式，且必须同时产出动作掩码（Masks）和基准动作（Baseline Actions）。这两者在“残差学习”（Residual Learning）框架中扮演着完全不同的角色。

#### **2.2.1 动作掩码（Action Mask）：硬约束的物理围栏**

动作掩码 $\\mathcal{M}\_{safe}$ 是一个布尔向量或 Logit 偏置，用于在物理层面禁止非法动作。  
假设当前时刻 $t$，代理拥有现金 $B\_t \= 1000$，空闲库容 $C\_{free} \= 50$。  
L1 计算出的掩码逻辑如下：

* **买入数量掩码**：$mask\_{qty} \= \[1, \\dots, 1, 0, \\dots, 0\]$。对于所有 $q \> 50$ 的动作索引，置为 0（不可选）。  
* **买入价格掩码**：对于选定的数量 $q$，任何 $p \> B\_t / q$ 的价格都被屏蔽。

这个掩码将作用于 L3 Decision Transformer 的 Softmax 层之前：

$$P(a|s) \= \\text{Softmax}(\\text{Logits}(s) \+ \\mathcal{M}\_{safe} \\cdot (-\\infty))$$

这确保了无论 L3 想要多么激进地探索，它永远无法执行自杀式操作。

#### **2.2.2 基准动作（Baseline Action）：残差学习的锚点**

这是 HRL-X 架构的核心。之所以需要 L1 输出基准动作 $a\_{base}$，是为了解决强化学习的\*\*样本效率（Sample Efficiency）\*\*问题。

SCML 的搜索空间巨大。如果让 L3 从零开始学习“如何买入”，它需要数百万步才能学会“价格越低越好”这一基本常识。  
通过引入 $a\_{base}$（例如 PenguinAgent 计算出的保守报价），我们将学习目标从 $f(s) \= a$ 变为 $f(s) \= \\Delta a$。

$$a\_{final} \= \\text{Clip}(a\_{base} \+ \\lambda \\cdot \\text{Tanh}(Output\_{L3}), \\mathcal{M}\_{safe})$$

* **训练初期**：神经网络输出接近 0，$\\lambda$ 较小。代理的表现 $\\approx a\_{base}$。这意味着代理在第 1 轮训练时就具备了冠军级别的生存能力。  
* **训练后期**：神经网络学会了在 $a\_{base}$ 的基础上微调。例如，L1 建议出价 100 元，L3 通过观察对手的历史序列，发现对手急于出货，于是输出 $\\Delta p \= \-5$，最终出价 95 元。

**结论**：L1 不需要 Transformer。它应该是用 Python 编写的高效逻辑代码。它必须输出基准动作，否则就放弃了残差学习带来的巨大优势，退化为普通的（且危险的）RL。

### **2.3 L1 与离线强化学习的交互机制**

用户疑惑：“**如果基于启发式代理，那么我们应该怎样使用离线强化学习？**”

这里的关键误区在于认为“离线 RL 是用来训练 L1 的”。事实上，**离线 RL 是用来训练 L3（以及 L2）去“修正”或“超越”L1 的**。

在离线训练流程中，L1 扮演的是**特征提取器（Feature Extractor）或数据预处理器**的角色。

1. **数据加载**：从日志中读取状态 $s$ 和专家实际采取的动作 $a\_{expert}$。  
2. **基准计算**：将 $s$ 输入 L1 启发式算法，计算出 $a\_{base}$。  
3. **残差提取**：计算目标残差 $\\Delta\_{target} \= a\_{expert} \- a\_{base}$。  
4. **模型更新**：训练 L3 网络，使其输入 $s$ 时，输出的预测残差 $\\Delta\_{pred}$ 尽可能接近 $\\Delta\_{target}$。

通过这种方式，L1 的“智慧”被硬编码在系统中，而 RL 只需要学习 L1 覆盖不到的“灵活性”。

## ---

**3\. 动态博弈对象的选择：L2 战略层与 L4 协调层的分工**

针对用户提出的第二个问题：“**L2层是如何选择谈判对象的？...还是说这个选择应该由L3层进行？如果是的话，L3层应该如何选择谈判对象？**”

这是一个触及多智能体系统（MAS）架构核心的问题。在 SCML 2025 的并发环境中，“选择谈判对象”并不是一个单一的决策步骤，而是一个\*\*分层过滤与资源分配（Hierarchical Filtering & Resource Allocation）\*\*的过程。

### **3.1 误区澄清：L2 不做点对点的选择**

在 HRL-X 架构中，L2 战略经理（Strategic Manager）运行在天（Day）的时间尺度上。它的视野是宏观的，看不到微观的“谈判对象 A”或“谈判对象 B”。  
L2 的职责是设定边界条件（Goal/Constraint Setting）。

#### **3.1.1 L2 的输出：战略目标向量**

L2 产生的向量 $g\_t$ 包含以下维度的指令 2：

$$g\_t \= \[Q\_{buy}^{target}, P\_{buy}^{limit}, Q\_{sell}^{target}, P\_{sell}^{limit}\]$$

* $Q\_{buy}^{target}$：今日需采购的总量（例如 50 单位）。  
* $P\_{buy}^{limit}$：采购的最高限价（例如 110 元）。

**隐式选择机制**：虽然 L2 没有指名道姓地选择对象，但通过设定 $P\_{buy}^{limit} \= 110$，它实际上**过滤**掉了所有要价高于 110 的谈判对象。这是一种基于经济属性的“被动选择”。

### **3.2 L3 的局限性：局部视角的盲目性**

L3 残差执行者（Residual Actor）运行在\*\*轮（Round）\*\*的时间尺度上，且每个 L3 实例被绑定在一个特定的谈判线程（Thread）中。

* 线程 A 的 L3 只能看到对手 A。  
* 线程 B 的 L3 只能看到对手 B。  
  因此，L3 无法选择谈判对象，因为它没有全局视角。它只能决定“如何”与当前的对手谈判（让步还是强硬），而不能决定“是否”应该把资源投入到这个对手身上。

### **3.3 L4 全局协调器：真正的选择者**

真正回答“如何选择谈判对象”这一问题的，是架构中的 **L4 全局协调器（Global Coordinator）**。它位于 L2 和 L3 之间，负责处理并发耦合（Concurrency Coupling）。

#### **3.3.1 基于注意力的资源分配机制**

L4 并不像传统的电话接线员那样“接通”某个对象，而是通过\*\*注意力权重（Attention Weights）\*\*来动态分配代理的“关注度”和“成交意愿”。

**输入（与当前实现对齐）**：L4 接收所有活跃谈判线程的**显式特征集合** `thread_feat_set`（每个线程一条）以及**全局上下文** `global_feat`。这些特征来自可观测状态与谈判历史（交期/角色、谈判进度、与桶目标差值、该交期的 `X_temporal[δ]` 切片、L1 baseline/约束等），**可离线重建**，用于避免依赖 L3 `hidden_state/latent` 带来的分布漂移与启发式教师不一致问题。

计算：L4 使用多头自注意力机制（Multi-Head Self-Attention）计算每个线程的权重 $\\alpha\_k$。

$$\\alpha \= \\text{Softmax}\\left( \\frac{Q\_{global} \\cdot K\_{threads}^T}{\\sqrt{d\_k}} \\right)$$

* $Q\_{global}$：由 L2 的目标向量 $g\_t$ 和当前全局状态（如剩余资金）编码而成。  
* $K\_{threads}$：各线程特征的键向量。

**选择逻辑（Selection Logic）**：

* **高权重（$\\alpha\_k \\to 1$）**：意味着该线程对实现 L2 的目标更关键。L4 会在该线程上施加更高的“成交意愿/资源优先级”（例如放大 residual，或在生成报价时优先占用预算/安全量）。**最终动作仍受 L1/L2 的裁剪约束**，不会无约束地突破安全边界。  
* **低权重（$\\alpha\_k \\to 0$）**：意味着该对手没有吸引力。L3 将收到抑制信号，采取拖延策略或直接给出苛刻的报价。

#### **3.3.2 动态性示例**

1. **初始时刻**：L2 设定买入 50 单位。L4 看到 10 个供应商，所有 $\\alpha\_k \\approx 0.1$。所有线程都在试探。  
2. **第 5 轮**：供应商 C 突然大幅降价。线程 C 的 `thread_feat`（报价偏差/可成交性等）发生变化。  
3. **重加权**：L4 重新计算，发现 $\\alpha\_C$ 飙升至 0.8，其他线程降至 0.02。  
4. **锁定**：系统“选择”了供应商 C，全力与其达成交易。一旦成交 50 单位，L2 的目标完成，L4 将关闭所有其他线程的注意力。

**结论**：选择谈判对象不是一个静态的决策，而是一个基于注意力机制的动态资源调度过程，由 L4 在微观层面实时执行，受 L2 的宏观目标指导。

## ---

**4\. 双层离线强化学习的训练范式**

针对用户提出的第三个问题：“**我们的离线强化学期究竟是如何分别让两层都进行学习的？两层的奖励信号分别是什么，我们如何从日志中获取状态空间和行动？**”

这是一个极具挑战性的工程问题，因为我们通常只有一个扁平化的日志文件（即 PenguinAgent 的交互记录），而我们需要从中训练出一个分层的系统。这需要使用\*\*事后经验回放（Hindsight Experience Replay, HER）\*\*的思想进行数据重构。

### **4.1 数据的分层剥离（Data Disentanglement）**

原始日志 negotiations.csv 包含了每一笔交易的细节。我们需要将其拆解为两套数据集：一套用于 L3（微观战术），一套用于 L2（宏观战略）。

#### **4.1.1 L3 微观数据集构建（Micro-Dataset）**

这是相对直接的。每一场谈判（Negotiation ID）就是一个独立的 Episode。

* **状态 $s\_{micro}$**：  
  * **上下文**：当前时间步 $t$，剩余谈判轮数。  
  * **序列历史**：对手过去 $N$ 轮的出价序列 $\\langle o\_{t-N}, \\dots, o\_{t-1} \\rangle$。  
  * **L2 指令上下文**：这里有一个难点。原始日志里没有 L2 指令。我们需要**假设** PenguinAgent 的指令就是它当时的某种行为模式（详见 4.2 节）。或者，在 Phase 0 阶段，我们简化 L3 的输入，暂不包含 Goal，只包含市场价格偏差。  
* **动作 $a\_{micro}$**：PenguinAgent 在该轮的具体出价 $(p, q)$。  
* **提取方法**：遍历 CSV，按 negotiation\_id 分组，按 round 排序。

#### **4.1.2 L2 宏观数据集构建（Macro-Dataset）：事后诸葛亮逻辑**

PenguinAgent 是基于规则的，它并没有显式地输出“今日目标”。为了训练 L2 模型去设定目标，我们必须从结果中反推目标。

* **逻辑**：如果 PenguinAgent 在第 $D$ 天结束时，总共买入了 80 单位原料，平均价格 105 元。那么我们可以认为，它在第 $D$ 天开始时的“潜意识目标”就是“买入 80 单位，限价 105”。  
* **状态 $s\_{macro}$**（取自第 $D$ 天开始时的快照）：  
  * 总库存 $I\_{total}$，总资金 $B\_{total}$。  
  * 市场平均价格指数 $P\_{index}$。  
  * 未来订单承诺量（Order Book）。  
* **动作 $a\_{macro}$（推断出的目标）**：  
  * $Q\_{target} \= \\sum\_{k \\in \\text{Deals}\_D} q\_k$  
  * $P\_{limit} \= \\max\_{k \\in \\text{Deals}\_D} p\_k$ （或者取加权平均 \+ 裕度）  
* **提取方法**：按 day 分组聚合 negotiations.csv 中的 deal 条目。

### **4.2 奖励信号工程（Reward Shaping）**

两层的奖励信号必须解耦，否则会导致目标错位。

#### **4.2.1 L3 执行层奖励：对齐与流动性**

L3 的任务不是“赚钱”（那是 L2 的责任），而是“高效执行 L2 的指令”和“在博弈中占据优势”。

$$R\_{L3} \= R\_{align} \+ R\_{advantage} \+ R\_{liquidity}$$

1. **对齐奖励 ($R\_{align}$)**：惩罚与 L2 目标（或 L1 基准）的偏差。  
   * 如果使用离线数据，这里实际上是 **行为克隆损失（BC Loss）**，即最小化 $(a\_{pred} \- a\_{log})^2$。  
2. **优势奖励 ($R\_{advantage}$)**：如果当前谈判的成交价比市场均价更有利，给予正反馈。  
   * $R\_{advantage} \= (P\_{market} \- P\_{deal}) \\times Q$ （对于买入）。  
3. **流动性奖励 ($R\_{liquidity}$)**：只要达成交易，给予微小的正向激励 $\\epsilon$。这对于防止离线 RL 产生的“保守冻结”现象（即不敢交易）至关重要。

#### **4.2.2 L2 战略层奖励：长期经济价值**

L2 对公司的生死存亡负责。它的奖励必须包含长期视角，解决信用分配问题。

$$R\_{L2} \= \\Delta \\text{Equity} \+ \\text{Potential}\_{\\Phi} \- \\text{Penalty}\_{risk}$$

1. **权益变动 ($\\Delta \\text{Equity}$)**：现金流的变化。但这有问题，买入会导致现金减少。  
2. 势能函数 ($\\text{Potential}\_{\\Phi}$) 1：这是解决 SCML 离线训练的核心技术。  
   * 定义势能 $\\Phi(s) \= \\text{Inventory} \\times P\_{market\\\_avg}$。  
   * L2 的奖励修正为：$R \= (B\_{t+1} \- B\_t) \+ \\gamma \\Phi(s\_{t+1}) \- \\Phi(s\_t)$。  
   * **效果**：当 L2 指令买入原料时，虽然 $B$ 减少了，但 $\\Phi$ 增加了，总奖励约为 0（如果是平价买入）。当 L2 指令卖出成品时，$B$ 增加，$\\Phi$ 减少，总奖励约为利润。这使得 L2 能够基于“价值”而非单纯的“现金”进行学习。  
3. **风险惩罚 ($\\text{Penalty}\_{risk}$)**：如果当天的库存水平导致了违约（Shortfall），给予巨大的负奖励。这迫使 L2 学会未雨绸缪。

### **4.3 具体的离线训练流程（Pipeline）**

1. **阶段 0：取证（Forensics）**  
   * 运行 1000 场包含 PenguinAgent 的比赛。  
   * 保存详细日志（不仅是 CSV，最好是序列化的 Python 对象以保留完整状态）。  
2. **阶段 1：L3 预训练（Pre-training L3）**  
   * 使用 L3 微观数据集。  
   * 采用 **Reward-on-the-Line (ROL)** 算法。训练 Decision Transformer 输入状态序列，预测 $Action\_{log} \- L1(State\_{log})$。  
   * **关键点**：仅过滤出最终产生正收益的谈判记录进行训练（Advantage Filtering）。  
3. **阶段 2：L2 预训练（Pre-training L2）**  
   * 使用 L2 宏观数据集。  
   * 采用 **BC (Behavior Cloning)** 或 **CQL (Conservative Q-Learning)**。  
   * 输入宏观状态，训练网络输出“推断出的 PenguinAgent 目标”。  
   * 这一步是为了让 L2 拥有一个“像专家一样思考”的初始策略（Warm Start）。  
4. **阶段 3：分层联合微调（Hierarchical Fine-tuning）**  
   * 这是一个**在线（Online）或半在线**的过程。  
   * 将预训练好的 L2 和 L3 放入仿真器。  
   * 冻结 L3，用 PPO 微调 L2：让 L2 尝试设定不同的目标，观察经过 L3 执行后的最终经济回报。利用势能奖励函数优化 L2，使其学会利用期货进行套利（这是 PenguinAgent 不擅长的，属于超越专家的部分）。

## ---

**5\. HRL-X 架构实施手册：数据结构与算法伪代码**

为了进一步具体化上述理论，本章提供核心组件的数据结构定义和关键算法逻辑。

### **5.1 数据结构定义**

#### **5.1.1 状态空间张量**

| 组件 | 张量名称 | 维度 (Batch, Dims) | 包含特征 | 来源 |
| :---- | :---- | :---- | :---- | :---- |
| **L2** | macro\_state | $(B, 32)$ | \[库存向量, 资金, 市场指数, 订单簿卷积特征\] | 每日快照 |
| **L3** | seq\_history | $(B, T=20, 16)$ |  | 谈判日志 |
| **L3** | context | $(B, 8)$ | \[剩余轮次, L2目标偏差, 市场波动率\] | 实时计算 |
| **L4** | thread\_feat\_set | $(B, K, 24)$ | 线程显式特征集合 | 可观测状态 + 谈判历史 |
| **L4** | global\_feat | $(B, 30)$ | 全局上下文特征 | `goal_hat` + `x_static` + 线程计数等 |

#### **5.1.2 动作空间与接口**

| 组件 | 输出变量 | 类型 | 物理含义 | 约束处理 |
| :---- | :---- | :---- | :---- | :---- |
| **L1** | mask | Binary Tensor | 禁止非法价格/数量 | 乘法应用于 Logits |
| **L1** | baseline | Continuous | 启发式建议报价 | 作为残差基准 |
| **L2** | goal\_vec | Continuous (4D) | \[买量, 买价限, 卖量, 卖价限\] | 经 Sigmoid 归一化后映射 |
| **L3** | residual | Continuous (2D) |  | 经 Tanh 缩放后加到基准 |
| **L4** | attn\_weights | Softmax Dist | 各线程调度权重 \(\\alpha\) | 用于批次调度与动态预留（买侧按 \(\\alpha\) 排序逐线程裁剪并扣减 `B_free/Q_safe`），也可用于 residual 门控 |

### **5.2 离线 RL 训练循环伪代码 (Python/PyTorch 风格)**

以下代码展示了如何在一个训练循环中同时处理 L1 基准和 L3 残差学习。

Python

def train\_l3\_offline(dataloader, l1\_heuristic, l3\_model, optimizer):  
    """  
    离线训练 L3 残差执行者  
    """  
    for batch in dataloader:  
        \# 1\. 解包数据  
        \# states: (B, T, F), actions\_expert: (B, 2\)  
        states, actions\_expert, masks \= batch  
          
        \# 2\. 计算 L1 基准 (无需梯度)  
        with torch.no\_grad():  
            \# L1 启发式通常是不可导的逻辑代码，这里假设已向量化或在预处理中完成  
            actions\_baseline \= l1\_heuristic(states\[:, \-1, :\])   
          
        \# 3\. 计算目标残差  
        \# Target \= Expert \- Baseline  
        residual\_target \= actions\_expert \- actions\_baseline  
          
        \# 4\. 前向传播 L3  
        \# L3 输出的是预测的残差  
        residual\_pred \= l3\_model(states)  
          
        \# 5\. 应用 ROL (Reward-on-the-Line) 约束  
        \# 假设我们有 N 个 Ensemble 模型，这里简化为一个  
        \# 核心是只学习 Expert 做得好的部分，或者简单地最小化 MSE  
        loss\_mse \= torch.nn.functional.mse\_loss(residual\_pred, residual\_target)  
          
        \# 6\. 安全性正则化 (Safety Regularization)  
        \# 惩罚那些加上基准后会被 Mask 掉的预测值  
        actions\_final \= actions\_baseline \+ residual\_pred  
        loss\_safety \= torch.mean(torch.relu(actions\_final \- masks.upper\_bound))   
          
        total\_loss \= loss\_mse \+ 0.1 \* loss\_safety  
          
        \# 7\. 反向传播  
        optimizer.zero\_grad()  
        total\_loss.backward()  
        optimizer.step()

### **5.3 L2 目标向量的特征工程**

为了从日志中提取 L2 的训练目标，我们需要编写一个“逆向工程”函数。

Python

def extract\_l2\_goals\_from\_logs(daily\_logs):  
    """  
    从每日日志中反推 L2 目标向量 (Hindsight Logic)  
    """  
    l2\_samples \=  
      
    for day\_log in daily\_logs:  
        \# 获取当天的宏观状态  
        state\_macro \= extract\_macro\_state(day\_log)  
          
        \# 统计当天实际发生的交易  
        deals \= day\_log\['deals'\]  
        if not deals:  
            \# 如果没交易，假设目标是  或者保持观望  
            \# 这里需要处理数据不平衡问题  
            target\_goal \=   
        else:  
            \# 买入统计  
            buy\_deals \= \[d for d in deals if d.type \== 'buy'\]  
            total\_buy\_qty \= sum(d.quantity for d in buy\_deals)  
            \# 价格限制推断：取当天成交的最高价作为“心理底线”  
            max\_buy\_price \= max(d.price for d in buy\_deals) if buy\_deals else 0  
              
            \# 卖出统计  
            sell\_deals \= \[d for d in deals if d.type \== 'sell'\]  
            total\_sell\_qty \= sum(d.quantity for d in sell\_deals)  
            min\_sell\_price \= min(d.price for d in sell\_deals) if sell\_deals else 0  
              
            target\_goal \= \[total\_buy\_qty, max\_buy\_price, total\_sell\_qty, min\_sell\_price\]  
              
        l2\_samples.append((state\_macro, target\_goal))  
          
    return l2\_samples

## ---

**6\. 系统集成与性能展望**

### **6.1 架构的整体协同效应**

通过上述设计，HRL-X 形成了一个有机的整体：

1. **L1** 提供了坚硬的底座，确保代理在任何情况下都不会“自杀”。  
2. **L2** 利用 PPO 和势能奖励，学会了跨越数十天的库存规划，解决了“不仅要活下去，还要活得久”的问题。  
3. **L3** 利用 Transformer 和离线残差学习，吸收了 PenguinAgent 的微观谈判技巧，并能够在 L1 的基础上进行微调，榨取每一分利润。  
4. **L4** 利用注意力机制，解决了并发环境下的“精神分裂”问题，确保所有谈判线程劲往一处使。

### **6.2 面对 2025 新规则的适应性**

* **非易腐库存**：L2 的势能奖励 $\\Phi(s)$ 直接将库存价值显性化，使得代理自然学会囤货策略。  
* **短缺惩罚**：L1 的掩码机制包含硬性的预留逻辑（$Q\_{min\\\_buy}$），在物理层面杜绝了大部分违约可能；L2 的风险惩罚进一步在战略层面规避高风险订单。

### **6.3 结论**

HRL-X 架构是对 SCML 2025 复杂环境的系统性回应。它超越了单一算法的局限，融合了规则系统的稳健性与深度学习的适应性。对于 L1、L2 设计及离线训练的具体实施，本报告提供的“屏蔽残差”、“注意力选择”及“事后反推训练”三大技术方案，构成了通往冠军代理的完整路径。

---

**附表：关键技术参数对照表**

| 模块 | 时间尺度 | 核心算法 | 输入 (S) | 输出 (A) | 训练方法 | 关键依赖 |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **L1** | 实时 | 启发式规则 (Penguin Logic) | $I\_t, B\_t$ | Mask, $a\_{base}$ | 无 (人工设计) | 领域知识 |
| **L2** | 天 (Day) | PPO / CQL | 宏观状态 $s\_{macro}$ | Goal Vector $g\_t$ | 离线 Hindsight \+ 在线 PPO | 势能函数 $\\Phi$ |
| **L3** | 轮 (Round) | Decision Transformer | 序列历史 $s\_{micro}$ | Residual $\\Delta a$ | 离线 ROL (Advantage Weighted) | L1 基准 |
| **L4** | 轮 (Round) | Attention Network | `thread_feat_set + global_feat` | Weights $\\alpha\_k$ | 启发式蒸馏 →（后续）在线微调 | 全局上下文 |

(报告结束)

#### **引用的著作**

1. HRL-X 研究：强化学习问题解决  
2. SCML 代理开发技术架构  
3. HRL-X 代理实现与数据收集
