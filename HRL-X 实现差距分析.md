# HRL-X è®¾è®¡ä¸å®ç°å·®è·åˆ†ææŠ¥å‘Š

> ç”Ÿæˆæ—¥æœŸï¼š2025å¹´12æœˆ12æ—¥  
> åˆ†æèŒƒå›´ï¼š5ä»½è®¾è®¡æ–‡æ¡£ vs `litaagent_std/hrl_x/` å®ç°ä»£ç 

---

## ğŸ“‹ æ€»ä½“è¯„ä¼°

å½“å‰å®ç°å¤„äº**éª¨æ¶é˜¶æ®µ**ï¼Œå¤§é‡æ ¸å¿ƒåŠŸèƒ½ä»…ä»¥å ä½ç¬¦å½¢å¼å­˜åœ¨ã€‚æ€»ä½“å®Œæˆåº¦çº¦ **15%**ã€‚

| ç»„ä»¶ | è®¾è®¡å®Œæˆåº¦ | çŠ¶æ€ |
|------|------------|------|
| L1 å®‰å…¨æŠ¤ç›¾ | ~30% | ğŸŸ¡ éƒ¨åˆ†å®ç° |
| L2 æˆ˜ç•¥ç®¡ç† | ~10% | ğŸ”´ ä»…å¯å‘å¼ |
| L3 æ®‹å·®æ‰§è¡Œ | ~5% | ğŸ”´ æ®‹å·®=0 |
| L4 å…¨å±€åè°ƒ | 0% | âš« å®Œå…¨ç¼ºå¤± |
| æ•°æ®æµæ°´çº¿ | ~25% | ğŸŸ¡ éª¨æ¶ |
| è®­ç»ƒæµç¨‹ | ~5% | ğŸ”´ ä»…å ä½ç¬¦ |
| ä¸»åŠ¨åå•† | 0% | âš« å®Œå…¨ç¼ºå¤± |

---

## ğŸ”´ ä¸¥é‡ç¼ºå¤±ï¼ˆæ ¸å¿ƒæ¶æ„ç»„ä»¶ï¼‰

### 1. L1 å®‰å…¨æŠ¤ç›¾å±‚ - é‡å¤§åŠŸèƒ½ç¼ºå¤±

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` ç¬¬4èŠ‚ã€`L1-L4 å±‚è®¾è®¡ä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ .md` ç¬¬2èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š

```python
# çº¦æŸä¸€ï¼šæœ€å¤§å®‰å…¨ä¹°å…¥é‡
Q_max_buy = C_total - I_current - I_incoming + O_committed

# çº¦æŸäºŒï¼šæœ€å°å¿…è¦ä¹°å…¥é‡
Q_min_buy = max(0, O_committed - I_current - I_incoming)

# çº¦æŸä¸‰ï¼šç ´äº§ä¿æŠ¤ä»·æ ¼
P_limit(q) = (B_t - Reserve) / q
```

- ç”Ÿæˆ**åŠ¨ä½œæ©ç å¼ é‡ï¼ˆAction Mask Tensorï¼‰**ï¼Œä½œç”¨äº L3 çš„ Softmax å±‚
- ç”Ÿæˆ**åŸºå‡†åŠ¨ä½œï¼ˆBaseline Actionï¼‰**ï¼š`a_base = (Q_min_buy, Cost_production Ã— (1 + Margin_min))`
- å®ç°ä¸º TensorFlow `SafetyMaskingLayer` è‡ªå®šä¹‰å±‚

**å½“å‰å®ç°**ï¼ˆ`l1_safety.py`ï¼‰ï¼š

```python
class PenguinMicroBaseline:
    def baseline_offer(self, target, delivery_time):
        qty = max(0, target.remaining)
        price = target.price_limit
        return qty, delivery_time, price

    def clip_offer(self, offer, wallet, target, is_buying, inventory_capacity):
        # ç®€å•è£å‰ªï¼Œæ— åˆçº¦çº¦æŸ
```

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] `I_incoming`ï¼ˆåœ¨é€”åŸææ–™ï¼‰è¿½è¸ªé€»è¾‘
- [ ] `O_committed`ï¼ˆå·²æ‰¿è¯ºè®¢å•ï¼‰è®¡ç®—é€»è¾‘
- [ ] åŸºäºå·²ç­¾åˆåŒé˜Ÿåˆ—çš„çº¦æŸè®¡ç®—
- [ ] åŠ¨ä½œæ©ç å¼ é‡ç”Ÿæˆï¼ˆ`mask_tensor`ï¼‰
- [ ] TensorFlow/PyTorch `SafetyMaskingLayer` ç±»
- [ ] ä¸ L3 è¾“å‡ºå±‚çš„ Logit Masking é›†æˆ
- [ ] ç ´äº§ä¿æŠ¤ä»·æ ¼çš„åŠ¨æ€è®¡ç®—

---

### 2. L2 æˆ˜ç•¥ç®¡ç†å±‚ - å‡ ä¹å®Œå…¨ç¼ºå¤±

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` ç¬¬5èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š

```python
class ManagerPPOAgent(tf.keras.Model):
    def __init__(self):
        # æœŸè´§æ‰¿è¯ºå‘é‡å·ç§¯å±‚
        self.future_conv = tf.keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu')
        # Actor ç½‘ç»œï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰
        self.actor_out = tf.keras.layers.Dense(action_dim * 2)  # mean + log_std
        # Critic ç½‘ç»œ
        self.critic_out = tf.keras.layers.Dense(1)
```

**çŠ¶æ€ç©ºé—´ `S_high`**ï¼š
- åº“å­˜åŠ¿èƒ½ç‰¹å¾ï¼š`Î¦(s) = I_total Ã— P_avg`
- èµ„é‡‘å¥åº·åº¦ï¼š`B_t / B_initial`
- æœŸè´§æ‰¿è¯ºå‘é‡ï¼šé•¿åº¦ä¸º Hï¼ˆå¦‚10å¤©ï¼‰çš„å‘é‡ï¼Œéœ€ **Conv1D** å¤„ç†
- è¿‡å»10å¤©çš„å¸‚åœºå‡ä»·ä¸æˆäº¤é‡è¶‹åŠ¿

**åŠ¨ä½œç©ºé—´**ï¼š
```python
g_t = [Q_target_buy, P_limit_buy, Q_target_sell, P_limit_sell]
```

**å½“å‰å®ç°**ï¼ˆ`agent.py`ï¼‰ï¼š

```python
def _heuristic_manager(self, obs):
    capacity = max(1, int(obs["capacity"]))
    need_buy = max(0, capacity - int(obs["inventory_in"]))
    buy_price = obs["market_price_in"] * 1.05
    # ... ç®€å•è§„åˆ™
```

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] `ManagerPPOAgent` ç±»ï¼ˆActor-Critic ç½‘ç»œï¼‰
- [ ] Conv1D å±‚å¤„ç†æœŸè´§æ‰¿è¯ºå‘é‡
- [ ] é«˜æ–¯åˆ†å¸ƒé‡‡æ ·æœºåˆ¶ï¼ˆ`tfp.distributions.Normal`ï¼‰
- [ ] PPO Clip æŸå¤±å‡½æ•°ï¼ˆ`compute_loss` æ–¹æ³•ï¼‰
- [ ] çŠ¶æ€ç‰¹å¾æ‰©å±•ï¼š
  - [ ] åº“å­˜åŠ¿èƒ½ `Î¦(s)` è®¡ç®—
  - [ ] æœªæ¥ H å¤©çš„è®¢å•æ‰¿è¯ºå‘é‡
  - [ ] è¿‡å» 10 å¤©å¸‚åœºè¶‹åŠ¿
  - [ ] èµ„é‡‘å¥åº·åº¦æ¯”ä¾‹
- [ ] ä»·å€¼å‡½æ•° (Critic) ç½‘ç»œ
- [ ] ç†µæ­£åˆ™åŒ–é¡¹

---

### 3. L3 æ®‹å·®æ‰§è¡Œå±‚ - æ ¸å¿ƒæœªå®ç°

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` ç¬¬6èŠ‚ã€`HRL-X ç ”ç©¶ï¼šå¼ºåŒ–å­¦ä¹ é—®é¢˜è§£å†³.md` ç¬¬3.3èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š

```python
class ResidualDecisionTransformer(tf.keras.Model):
    def __init__(self, d_model=128, n_heads=4, n_layers=2, max_len=20, action_dim=2):
        # çŠ¶æ€åµŒå…¥
        self.state_emb = tf.keras.layers.Dense(d_model)
        # ç›®æ ‡åµŒå…¥ï¼ˆL2 Goal æ³¨å…¥ï¼‰
        self.goal_emb = tf.keras.layers.Dense(d_model)
        # ä½ç½®ç¼–ç 
        self.pos_emb = tf.keras.layers.Embedding(max_len, d_model)
        # Transformer Blocks
        self.blocks = [...]  # MultiHeadAttention, FFN, LayerNorm
        # æ®‹å·®è¾“å‡ºå¤´
        self.action_head = tf.keras.layers.Dense(action_dim, activation='tanh')
        # å¯å­¦ä¹ ç¼©æ”¾å› å­
        self.residual_scale = tf.Variable([5.0, 10.0], trainable=True)
```

**æ ¸å¿ƒæœºåˆ¶**ï¼š
```python
A_final = Clip(A_base + Î”a, M_safe)
```

- è¾“å…¥ï¼šè°ˆåˆ¤å†å²åºåˆ— `H_k = {o_{t-N}, ..., o_t}` + L2 ç›®æ ‡å‘é‡ `g_t`
- å› æœæ©ç ï¼ˆCausal Maskï¼‰é˜²æ­¢ä¿¡æ¯æ³„éœ²
- è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰å¯¹æ‰‹è¡Œä¸ºæ¨¡å¼

**å½“å‰å®ç°**ï¼ˆ`agent.py`ï¼‰ï¼š

```python
def respond(self, negotiator_id, state):
    # ...
    baseline = self.l1.baseline_offer(target, ...)
    clipped = self.l1.clip_offer(baseline, ...)
    # æ®‹å·® = 0ï¼Œç›´æ¥è¿”å›åŸºå‡†
    return SAOResponse(ResponseType.REJECT_OFFER, clipped)
```

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] `ResidualDecisionTransformer` ç±»
- [ ] Transformer Blocksï¼ˆMultiHeadAttention, FFN, LayerNormï¼‰
- [ ] çŠ¶æ€åµŒå…¥å±‚ï¼ˆ`state_emb`ï¼‰
- [ ] ç›®æ ‡æ¡ä»¶æ³¨å…¥æœºåˆ¶ï¼ˆ`goal_emb`ï¼‰
- [ ] ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰
- [ ] å› æœæ©ç ï¼ˆCausal Maskï¼‰å®ç°
- [ ] å¯å­¦ä¹  `residual_scale` å‚æ•°
- [ ] è°ˆåˆ¤å†å²åºåˆ—çš„æ»‘åŠ¨çª—å£ç®¡ç†
- [ ] éšçŠ¶æ€ `h_k` è¾“å‡ºï¼ˆä¾› L4 ä½¿ç”¨ï¼‰

---

### 4. L4 å…¨å±€åè°ƒå±‚ - å®Œå…¨ç¼ºå¤± âš«

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`PenguinAgent ç›®æ ‡ä¸è°ˆåˆ¤æœºåˆ¶.md` ç¬¬4èŠ‚ã€`HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` ç¬¬7èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š

```python
class GlobalCoordinator(tf.keras.layers.Layer):
    def __init__(self, d_model=64, n_heads=4):
        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)
        self.dense_q = tf.keras.layers.Dense(d_model)  # Query from global state
        self.dense_k = tf.keras.layers.Dense(d_model)  # Key from thread states
```

**æ ¸å¿ƒæœºåˆ¶**ï¼š
```python
# è¾“å…¥ï¼šæ‰€æœ‰æ´»è·ƒ L3 çº¿ç¨‹çš„éšçŠ¶æ€
H_in = {h_1, h_2, ..., h_K}

# æ³¨æ„åŠ›æƒé‡è®¡ç®—
Î± = Softmax(Q @ K^T / âˆšd_k)

# è¾“å‡ºï¼šçº¿ç¨‹é‡è¦æ€§æƒé‡
# Î±_k é«˜ â†’ L3 å˜å¾—æ¿€è¿›ï¼Œç¡®ä¿æˆäº¤
# Î±_k ä½ â†’ L3 å˜å¾—ä¿å®ˆï¼Œå¯æ”¾å¼ƒ
```

**å½“å‰å®ç°**ï¼š

**å®Œå…¨ä¸å­˜åœ¨ä»»ä½• L4 ç›¸å…³ä»£ç **

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] `GlobalCoordinator` ç±»
- [ ] å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—
- [ ] å…¨å±€çŠ¶æ€ç¼–ç ï¼ˆQuery ç”Ÿæˆï¼‰
- [ ] çº¿ç¨‹éšçŠ¶æ€æ”¶é›†æœºåˆ¶
- [ ] æ³¨æ„åŠ›æƒé‡ `Î±_k` åˆ†é…
- [ ] æƒé‡åˆ° L3 ç­–ç•¥è°ƒåˆ¶çš„æ˜ å°„
- [ ] ç«¯åˆ°ç«¯è®­ç»ƒé›†æˆï¼ˆä¸ L3 è”åˆåå‘ä¼ æ’­ï¼‰
- [ ] å¹¶å‘èµ„æºäº‰å¤ºè§£å†³æœºåˆ¶

---

### 5. ä¸»åŠ¨åå•†å‘èµ·æœºåˆ¶ - å®Œå…¨ç¼ºå¤± âš«

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`PenguinAgent ç›®æ ‡ä¸è°ˆåˆ¤æœºåˆ¶.md` ç¬¬3èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š**"å¹¿æ’­-è¿‡æ»¤"ï¼ˆBroadcast-Filterï¼‰åè®®**

```python
def before_step(self):
    # 1. L2 è®¾å®šç›®æ ‡
    goals = self.l2_manager.predict(macro_state)
    
    # 2. è·å–æ‰€æœ‰æ½œåœ¨ä¼™ä¼´
    partners = self.awi.my_suppliers  # æˆ– my_consumers
    
    # 3. å‘æ‰€æœ‰ä¼™ä¼´å‘èµ·åå•†è¯·æ±‚ï¼ˆé¥±å’Œå¼è¯·æ±‚ï¼‰
    for partner in partners:
        self.awi.request_negotiation(
            partner=partner,
            product=self.awi.my_input_products[0],
            quantity=goals.Q_target_buy,
            unit_price=goals.P_limit_buy,
            time=self.awi.current_step + 1,
        )
```

**æ ¸å¿ƒé€»è¾‘**ï¼š
- ä¸ä¸»è§‚æŒ‘é€‰å¯¹è±¡ï¼Œå‘**æ‰€æœ‰**æ½œåœ¨ä¾›åº”å•†/æ¶ˆè´¹è€…å¹¿æ’­
- ç”± L4 åœ¨åå•†è¿‡ç¨‹ä¸­åŠ¨æ€å†³å®šå“ªäº›çº¿ç¨‹å€¼å¾—æˆäº¤
- æ€»è¯·æ±‚é‡ = N Ã— Q_targetï¼ˆæœ‰è¿‡åº¦æ‰¿è¯ºé£é™©ï¼Œéœ€ L4 è§£å†³ï¼‰

**å½“å‰å®ç°**ï¼ˆ`agent.py`ï¼‰ï¼š

```python
def before_step(self):
    super().before_step()
    self._ensure_targets()
    state_h = self._macro_obs()
    goals = self._heuristic_manager(state_h)
    self.buy_target = DailyTarget(...)
    self.sell_target = DailyTarget(...)
    # âŒ æ²¡æœ‰ä¸»åŠ¨å‘èµ·åå•†ï¼Œå®Œå…¨è¢«åŠ¨ç­‰å¾…
```

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] `awi.request_negotiation()` è°ƒç”¨
- [ ] æ½œåœ¨ä¼™ä¼´åˆ—è¡¨è·å–ï¼ˆ`awi.my_suppliers` / `awi.my_consumers`ï¼‰
- [ ] é¥±å’Œå¼åå•†è¯·æ±‚å‘èµ·
- [ ] ä¸ L4 è¿‡æ»¤æœºåˆ¶çš„é…åˆ

---

## ğŸŸ  è®­ç»ƒæµç¨‹ç¼ºå¤±

### 6. ç¦»çº¿å¼ºåŒ–å­¦ä¹  - ä»…æœ‰éª¨æ¶

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`L1-L4 å±‚è®¾è®¡ä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ .md` ç¬¬4èŠ‚ã€`HRL-X ç ”ç©¶ï¼šå¼ºåŒ–å­¦ä¹ é—®é¢˜è§£å†³.md` ç¬¬5èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š**ROL (Reward-on-the-Line)** ç®—æ³•

```python
# é›†åˆä¸€è‡´æ€§ï¼šè®­ç»ƒ N=5 ä¸ª Q ç½‘ç»œ
Q_ensemble = [Q_1, Q_2, ..., Q_5]

# ä¸ç¡®å®šæ€§æƒ©ç½š
Q_target(s, a) = min_i Q_i(s, a) - Î» Ã— Var(Q_i(s, a))

# ä¼˜åŠ¿åŠ æƒè¡Œä¸ºå…‹éš†
L_ROL = ||a_pred - a_expert||Â² + Î»_var Ã— Var(Q_ensemble(s, a))
```

**å½“å‰å®ç°**ï¼ˆ`training.py`ï¼‰ï¼š

```python
class SimpleRegressor:
    def __init__(self, input_dim, output_dim):
        self.W = rng.standard_normal((input_dim, output_dim)) * 0.01
        self.b = np.zeros(output_dim)
    
    def fit(self, x, y, lr, epochs):
        for _ in range(epochs):
            pred = self.predict(x)
            grad = (pred - y) / len(x)
            self.W -= lr * x.T @ grad
```

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] ROL ç®—æ³•æ ¸å¿ƒå®ç°
- [ ] Q ç½‘ç»œé›†åˆï¼ˆEnsembleï¼‰
- [ ] ä¸ç¡®å®šæ€§æƒ©ç½šæœºåˆ¶
- [ ] ä¼˜åŠ¿åŠ æƒè¡Œä¸ºå…‹éš†
- [ ] PyTorch/TensorFlow æ·±åº¦å­¦ä¹ æ¨¡å‹æ›¿æ¢
- [ ] CQL (Conservative Q-Learning) å¤‡é€‰å®ç°
- [ ] æ•°æ®é›†ä¼˜åŠ¿è¿‡æ»¤ï¼ˆåªæ¨¡ä»¿ A(s,a) > 0 çš„æ ·æœ¬ï¼‰

---

### 7. åˆ†å±‚è”åˆå¾®è°ƒ - æœªå®ç°

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` ç¬¬8.3èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š**MAPPO (Multi-Agent PPO)** + å¤åˆå¥–åŠ±å‡½æ•°

```python
R_t = R_profit + Î»1 Ã— R_liquidity - Î»2 Ã— R_risk + Î»3 Ã— R_intrinsic
```

**å¥–åŠ±åˆ†é‡**ï¼š

| åˆ†é‡ | å…¬å¼ | ä½œç”¨ |
|------|------|------|
| `R_profit` | `(B_{t+1} - B_t) + Î³Î¦(s_{t+1}) - Î¦(s_t)` | åŠ¿èƒ½å‡½æ•°è§£å†³çŸ­è§†é—®é¢˜ |
| `R_liquidity` | `Îµ if deal else 0` | é˜²æ­¢ç­–ç•¥å†»ç»“ |
| `R_risk` | `-Î² Ã— exp(max(0, -I_future))` | å‰ç»æ€§é£é™©æƒ©ç½š |
| `R_intrinsic` | `-||q_executed - q_goal||Â²` | L3 ä¸ L2 ç›®æ ‡å¯¹é½ |

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] MAPPO ç®—æ³•å®ç°
- [ ] CTDEï¼ˆé›†ä¸­å¼è®­ç»ƒï¼Œå»ä¸­å¿ƒåŒ–æ‰§è¡Œï¼‰æ¶æ„
- [ ] åŠ¿èƒ½å‡½æ•° `Î¦(s) = I Ã— P_avg` è®¡ç®—
- [ ] å¤åˆå¥–åŠ±å‡½æ•°å„åˆ†é‡å®ç°
- [ ] L2/L3/L4 è”åˆè®­ç»ƒå¾ªç¯
- [ ] GAE (Generalized Advantage Estimation)

---

### 8. è‡ªåšå¼ˆè®­ç»ƒ - æœªå®ç°

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` ç¬¬8.4èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š

```python
# å¯¹æ‰‹æ± 
opponent_pool = [
    PenguinAgent,           # é™æ€åŸºå‡†
    AS0,                    # é™æ€åŸºå‡†
    LitaAgentHRL_v1,        # å†å²ç‰ˆæœ¬
    LitaAgentHRL_v2,        # å†å²ç‰ˆæœ¬
    ...
]

# è®­ç»ƒå¾ªç¯
for epoch in training:
    opponent = random.choice(opponent_pool)
    run_episode(current_agent, opponent)
    if epoch % save_interval == 0:
        opponent_pool.append(copy(current_agent))
```

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] å¯¹æ‰‹æ± ç®¡ç†æœºåˆ¶
- [ ] æ¨¡å‹ç‰ˆæœ¬ä¿å­˜ä¸åŠ è½½
- [ ] éšæœºå¯¹æ‰‹é‡‡æ ·
- [ ] çº³ä»€å‡è¡¡é€¼è¿‘è¯„ä¼°

---

## ğŸŸ¡ æ•°æ®æµæ°´çº¿ç¼ºå¤±

### 9. å®è§‚ç›®æ ‡å–è¯é‡æ„ - ä¸å®Œæ•´

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`PenguinAgent ç›®æ ‡ä¸è°ˆåˆ¤æœºåˆ¶.md` ç¬¬2èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š

```python
# ä¹°å…¥é‡ç›®æ ‡é‡æ„
Q_target_buy = min(Q_max_safe, Q_needed)

where:
    Q_max_safe = C_total - I_current - I_incoming + O_committed
    Q_needed = max(0, Î£ D_future - I_current - I_incoming)

# ä¹°å…¥é™ä»·é‡æ„
P_limit_buy = max(å½“å¤©æ‰€æœ‰å‡ºä»·)  # æˆ– P_market_sell - C_process - Margin
```

**éœ€è¦ä»æ—¥å¿—æå–**ï¼š
- åœ¨é€”åŸææ–™ `I_incoming`
- å·²æ‰¿è¯ºè®¢å• `O_committed`
- æœªæ¥ H å¤©çš„é”€å”®åˆåŒéœ€æ±‚é‡ `D_future`
- `world_stats.csv` ä¸­çš„çŠ¶æ€å¿«ç…§

**å½“å‰å®ç°**ï¼ˆ`data_pipeline.py`ï¼‰ï¼š

```python
def build_macro_dataset(df):
    grouped = df[df["response"] == "accept"].groupby("time")
    for day, g in grouped:
        buy_qty = int(buy_deals["quantity"].sum())
        buy_price = float(buy_deals["price"].max())
        # âŒ ç®€å•èšåˆï¼Œæ— åˆçº¦é€†å‘å·¥ç¨‹
```

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] `I_incoming` è¿½è¸ªï¼ˆå·²ç­¾æœªäº¤ä»˜åˆåŒï¼‰
- [ ] `O_committed` è®¡ç®—ï¼ˆå·²ç­¾é”€å”®åˆåŒï¼‰
- [ ] æœªæ¥ H å¤©è®¢å•éœ€æ±‚é‡æå–
- [ ] `world_stats.csv` / `contracts.csv` è§£æ
- [ ] å®Œæ•´å®è§‚çŠ¶æ€ç‰¹å¾æå–ï¼š
  - [ ] å½“æ—¥åº“å­˜æ°´å¹³
  - [ ] å½“æ—¥èµ„é‡‘ä½™é¢
  - [ ] å¸‚åœºä»·æ ¼æŒ‡æ•°
  - [ ] ç”Ÿäº§çº¿é—²ç½®ç‡

---

### 10. å¾®è§‚åºåˆ—æ•°æ®é›† - è¿‡äºç®€åŒ–

**è®¾è®¡æ–‡ä»¶å¼•ç”¨**ï¼š`HRL-X ä»£ç†å®ç°ä¸æ•°æ®æ”¶é›†.md` ç¬¬3.3èŠ‚

**è®¾è®¡è¦æ±‚**ï¼š

| å­—æ®µ | å«ä¹‰ | ç”¨é€” |
|------|------|------|
| `id` | è°ˆåˆ¤å”¯ä¸€æ ‡è¯† | å…³è”ä¸åŒè½®æ¬¡ |
| `round` | è°ˆåˆ¤è½®æ¬¡ | è®©æ­¥æ›²çº¿åˆ†æ |
| `offer` | æè®® `(q, t, p)` | æ¨¡ä»¿å­¦ä¹ ç›®æ ‡ |
| `response` | å›åº”ç±»å‹ | å­¦ä¹ æ¥å—é˜ˆå€¼ |
| `time` | å“åº”æ—¶é—´ | å¯¹æ‰‹æ€¥è¿«åº¦æ¨æ–­ |

**éœ€è¦æå–**ï¼š
- å®Œæ•´çš„å‡ºä»·åºåˆ—ï¼ˆä¾› Transformer è¾“å…¥ï¼‰
- æ¯è½®å“åº”æ—¶é—´
- è®©æ­¥æ›²çº¿ï¼ˆConcession Curveï¼‰
- L1 åŸºå‡†çš„é¢„è®¡ç®—

**å½“å‰å®ç°**ï¼š

```python
def build_micro_dataset(df):
    for nid, g in df.groupby("id"):
        g_sorted = g.sort_values("round")
        last = g_sorted.iloc[-1]
        action = {"quantity": last.get("quantity"), ...}
        # âŒ ä»…å–æœ€åä¸€è½®ï¼Œä¸¢å¤±åºåˆ—ä¿¡æ¯
```

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] å®Œæ•´å†å²åºåˆ—ä¿ç•™ï¼ˆä¸åªæ˜¯æœ€åä¸€è½®ï¼‰
- [ ] å“åº”æ—¶é—´ç‰¹å¾æå–
- [ ] è®©æ­¥æ›²çº¿è®¡ç®—ï¼ˆ`price[t] - price[t-1]`ï¼‰
- [ ] L1 åŸºå‡†é¢„è®¡ç®—å¹¶å­˜å‚¨åœ¨æ ·æœ¬ä¸­
- [ ] åºåˆ—æˆªæ–­/å¡«å……åˆ°å›ºå®šé•¿åº¦ï¼ˆä¾› Transformer ä½¿ç”¨ï¼‰

---

## ğŸ”µ Agent å®ç°ç»†èŠ‚é—®é¢˜

### 11. çŠ¶æ€è§‚æµ‹ä¸å®Œæ•´

**è®¾è®¡è¦æ±‚ vs å½“å‰å®ç°å¯¹æ¯”**ï¼š

| ç‰¹å¾ | è®¾è®¡è¦æ±‚ | å½“å‰å®ç° | çŠ¶æ€ |
|------|----------|----------|------|
| `step_progress` | âœ“ | âœ“ | âœ… |
| `balance` | âœ“ | âœ“ | âœ… |
| `inventory_in` | âœ“ | âœ“ | âœ… |
| `inventory_out` | âœ“ | âœ“ | âœ… |
| `market_price_in` | âœ“ | âœ“ | âœ… |
| `market_price_out` | âœ“ | âœ“ | âœ… |
| `capacity` | âœ“ | âœ“ | âœ… |
| **æœŸè´§æ‰¿è¯ºå‘é‡** | âœ“ | âŒ | ğŸ”´ ç¼ºå¤± |
| **å¸‚åœºå†å²è¶‹åŠ¿** | âœ“ | âŒ | ğŸ”´ ç¼ºå¤± |
| **å·²ç­¾åˆåŒé˜Ÿåˆ—** | âœ“ | âŒ | ğŸ”´ ç¼ºå¤± |
| **åº“å­˜åŠ¿èƒ½ Î¦(s)** | âœ“ | âŒ | ğŸ”´ ç¼ºå¤± |
| **èµ„é‡‘å¥åº·åº¦æ¯”ä¾‹** | âœ“ | âŒ | ğŸ”´ ç¼ºå¤± |

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] æœªæ¥ H å¤©æœŸè´§æ‰¿è¯ºå‘é‡
- [ ] è¿‡å» 10 å¤©å¸‚åœºè¶‹åŠ¿ï¼ˆå‡ä»·ã€æˆäº¤é‡ï¼‰
- [ ] å·²ç­¾åˆåŒä¿¡æ¯ï¼ˆä¹°å…¥/å–å‡ºé˜Ÿåˆ—ï¼‰
- [ ] åº“å­˜åŠ¿èƒ½ `Î¦(s) = I Ã— P_avg`
- [ ] èµ„é‡‘å¥åº·åº¦ `B_t / B_initial`

---

### 12. ä½å±‚çŠ¶æ€è§‚æµ‹ç¼ºå¤±

**è®¾è®¡è¦æ±‚çš„ `S_low`**ï¼š

| ç‰¹å¾ | å½“å‰çŠ¶æ€ |
|------|----------|
| `subgoal_remaining` | ğŸŸ¡ æœ‰ `target.remaining`ï¼Œæœªå½’ä¸€åŒ– |
| `negotiation_time` | ğŸ”´ ç¼ºå¤± |
| `current_offer` | âœ… å·²æœ‰ |
| `opponent_history` | ğŸ”´ ç¼ºå¤± |

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] è°ˆåˆ¤å‰©ä½™æ—¶é—´/è½®æ¬¡ç‰¹å¾
- [ ] å½’ä¸€åŒ–çš„å­ç›®æ ‡å‰©ä½™é‡ `remaining / target`
- [ ] å¯¹æ‰‹å†å²å‡ºä»·åºåˆ—ç¼“å­˜

---

### 13. ä¹è§‚å¹¶å‘æ§åˆ¶å¢å¼º

**å½“å‰å®ç°**ï¼š

```python
if price_ok and qty <= target.remaining:
    target.register_deal(qty)
    return SAOResponse(ResponseType.ACCEPT_OFFER, offer)
```

**å­˜åœ¨é—®é¢˜**ï¼š
- è™½ç„¶æœ‰ `register_deal`ï¼Œä½†æ²¡æœ‰ä¸ L4 é…åˆ
- æ— æ³•åœ¨å¤šçº¿ç¨‹é—´åè°ƒèµ„æºåˆ†é…

**ç¼ºå¤±æ¸…å•**ï¼š

- [ ] ä¸ L4 åè°ƒå™¨çš„é›†æˆ
- [ ] è·¨çº¿ç¨‹èµ„æºåˆ†é…æœºåˆ¶
- [ ] è¿‡åº¦æ‰¿è¯ºå›æ»šé€»è¾‘ï¼ˆå½“åˆåŒå¤±è´¥æ—¶ï¼‰

---

## ğŸ¯ å®æ–½ä¼˜å…ˆçº§å»ºè®®

### P0 - æœ€é«˜ä¼˜å…ˆçº§ï¼ˆæ¶æ„æ ¸å¿ƒï¼‰

1. **å®ç° L4 å…¨å±€åè°ƒå±‚** - è§£å†³å¹¶å‘èµ„æºè€¦åˆ
2. **å®ç° L3 Decision Transformer** - æ ¸å¿ƒæ™ºèƒ½èƒ½åŠ›
3. **å®ç°ä¸»åŠ¨åå•†å‘èµ·ï¼ˆå¹¿æ’­-è¿‡æ»¤åè®®ï¼‰** - æ‰“ç ´è¢«åŠ¨æ¨¡å¼

### P1 - é«˜ä¼˜å…ˆçº§

4. **å®Œå–„ L1 åŠ¨ä½œæ©ç å’Œåˆçº¦çº¦æŸ** - å®‰å…¨åŸºç¡€
5. **å®ç° L2 PPO æˆ˜ç•¥ç®¡ç†å™¨** - è·¨æœŸè§„åˆ’
6. **æ‰©å±•çŠ¶æ€è§‚æµ‹ç©ºé—´** - ä¿¡æ¯å®Œå¤‡æ€§

### P2 - ä¸­ä¼˜å…ˆçº§

7. **å®Œå–„æ•°æ®æµæ°´çº¿** - è®­ç»ƒæ•°æ®è´¨é‡
8. **å®ç° ROL ç¦»çº¿ RL** - å†·å¯åŠ¨è§£å†³æ–¹æ¡ˆ
9. **å®ç°å¤åˆå¥–åŠ±å‡½æ•°** - è®­ç»ƒä¿¡å·

### P3 - åç»­ä¼˜åŒ–

10. **å®ç° MAPPO åœ¨çº¿è®­ç»ƒ**
11. **å®ç°è‡ªåšå¼ˆæœºåˆ¶**
12. **æ€§èƒ½è°ƒä¼˜ä¸è¶…å‚æœç´¢**

---

## ğŸ“‚ æ–‡ä»¶å˜æ›´æ¸…å•

éœ€è¦æ–°å»º/å¤§æ”¹çš„æ–‡ä»¶ï¼š

| æ–‡ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| `hrl_x/l4_coordinator.py` | ğŸ†• æ–°å»º | å…¨å±€åè°ƒå±‚ |
| `hrl_x/l3_transformer.py` | ğŸ†• æ–°å»º | Decision Transformer |
| `hrl_x/l2_manager.py` | ğŸ†• æ–°å»º | PPO æˆ˜ç•¥ç®¡ç†å™¨ |
| `hrl_x/l1_safety.py` | ğŸ”„ å¤§æ”¹ | æ·»åŠ æ©ç ã€åˆçº¦è¿½è¸ª |
| `hrl_x/agent.py` | ğŸ”„ å¤§æ”¹ | é›†æˆ L2/L3/L4ï¼Œæ·»åŠ ä¸»åŠ¨åå•† |
| `hrl_x/data_pipeline.py` | ğŸ”„ å¤§æ”¹ | å®Œå–„ç‰¹å¾æå– |
| `hrl_x/training.py` | ğŸ”„ å¤§æ”¹ | å®ç° ROL/MAPPO |
| `hrl_x/rewards.py` | ğŸ†• æ–°å»º | å¤åˆå¥–åŠ±å‡½æ•° |
| `hrl_x/self_play.py` | ğŸ†• æ–°å»º | è‡ªåšå¼ˆè®­ç»ƒ |

---

## ğŸ“ é™„å½•ï¼šè®¾è®¡æ–‡æ¡£é€ŸæŸ¥

| è®¾è®¡ç‚¹ | ä¸»è¦å‚è€ƒæ–‡æ¡£ |
|--------|--------------|
| L1 å®‰å…¨æŠ¤ç›¾ | `HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` Â§4, `L1-L4 å±‚è®¾è®¡ä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ .md` Â§2 |
| L2 æˆ˜ç•¥ç®¡ç† | `HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` Â§5 |
| L3 æ®‹å·®æ‰§è¡Œ | `HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` Â§6, `HRL-X ç ”ç©¶ï¼šå¼ºåŒ–å­¦ä¹ é—®é¢˜è§£å†³.md` Â§3.3 |
| L4 å…¨å±€åè°ƒ | `PenguinAgent ç›®æ ‡ä¸è°ˆåˆ¤æœºåˆ¶.md` Â§4, `HRL-X æ¶æ„å®ç°ä¸è®­ç»ƒæ–¹æ¡ˆ.md` Â§7 |
| ä¸»åŠ¨åå•† | `PenguinAgent ç›®æ ‡ä¸è°ˆåˆ¤æœºåˆ¶.md` Â§3 |
| ç¦»çº¿ RL | `L1-L4 å±‚è®¾è®¡ä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ .md` Â§4, `HRL-X ç ”ç©¶ï¼šå¼ºåŒ–å­¦ä¹ é—®é¢˜è§£å†³.md` Â§5 |
| å¥–åŠ±å‡½æ•° | `HRL-X ç ”ç©¶ï¼šå¼ºåŒ–å­¦ä¹ é—®é¢˜è§£å†³.md` Â§6 |
| æ•°æ®æµæ°´çº¿ | `HRL-X ä»£ç†å®ç°ä¸æ•°æ®æ”¶é›†.md` Â§3.3, `PenguinAgent ç›®æ ‡ä¸è°ˆåˆ¤æœºåˆ¶.md` Â§2 |

---

*æœ¬æ–‡æ¡£åº”éšå®ç°è¿›åº¦æ›´æ–°ï¼Œå®Œæˆé¡¹è¯·æ‰“å‹¾ âœ…*
